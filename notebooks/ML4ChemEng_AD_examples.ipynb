{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpGuLGUEcQsW"
      },
      "outputs": [],
      "source": [
        "# Authors:\n",
        "# Eduardo Iraola                              https://iraola.github.io/\n",
        "# Antonio del Rio Chanona                     https://www.imperial.ac.uk/people/a.del-rio-chanona\n",
        "\n",
        "# Optimisation and Machine Learning for Process Systems Engineering:\n",
        "# https://www.imperial.ac.uk/optimisation-and-machine-learning-for-process-engineering/about-us/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anomaly Detection Algorithms - Machine Learning for Chemical Engineering\n",
        "* Anomalous data is scarce, therefore in this type of problems we can only use normal operation data for training. The few instances of fault behavior we have are left for cross-validation purposes.\n",
        "* We face a semisupervised problem. We know the labels of normal instances only and we only feed the training with them. The distinction between both classes must be made *after* and not *during* training through some kind of threshold tuning.\n",
        "* Typical methods used range from statistical, neural networks, distance-based, and other unsupervised and semi-supervised methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1l92at3rAqU"
      },
      "source": [
        "# 1. Warm-up: Probabilistic approach with a single variable example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu6vni0jrKAw"
      },
      "source": [
        "In this example, we are generating temperature data from a chemical reactor. We have two sets of data:\n",
        "\n",
        "- The training set, which we are certain contains normal operation conditions (NOC) only\n",
        "- The cross-validation set, which contains a mix of both normal and anomaly conditions\n",
        "\n",
        "We will use statistical methods to try to predict the state of the system in the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "np.random.seed(52)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Data generation\n",
        "\n",
        "We will generate the data using the `gen_monovariate_data` function. Internally, this function calls `gen_data_train` and `gen_data_test` to generate the training and testing data sets, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialization\n",
        "mean = 400           # Mean of the normal distribution\n",
        "std = 3              # Standard deviation of the normal distribution\n",
        "n_noc_test = 100     # Number of test samples\n",
        "n_fault_slice=50     # Number of data points in the fault slice\n",
        "n_slice=10           # Number of slices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "6W5Tl10Ntve1",
        "outputId": "fa7efb17-85ac-48f7-8b3f-eb594433a571"
      },
      "outputs": [],
      "source": [
        "def gen_data_train(mean, std, n_noc=500):\n",
        "    # Generate normal operation conditions (NOC) data\n",
        "    X_train = np.random.normal(mean, std, n_noc)    \n",
        "    return X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_data_test(mean, std, n_noc=100, n_fault_slice=50, n_slice=10):\n",
        "    # Generate anomaly data with drift. 50 slices of 10 points each\n",
        "    # Total numer of samples is = n_noc + n_fault_slice * n_slice\n",
        "    changeRate = 1.02\n",
        "    X_test = np.random.normal(mean, std, n_noc)\n",
        "\n",
        "    poisson_variable = np.random.poisson(0.05, n_fault_slice)\n",
        "    mu_fault = mean\n",
        "    sigma_fault = std\n",
        "    for i in range(0, n_fault_slice):\n",
        "        mu_fault = mu_fault * (changeRate ** poisson_variable[i])\n",
        "        sigma_fault = sigma_fault * (changeRate ** poisson_variable[i])\n",
        "        next_fault_data = np.random.normal(mu_fault, sigma_fault, n_slice)\n",
        "        X_test = np.concatenate((X_test, next_fault_data)) \n",
        "          \n",
        "    return X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_monovariate_data(mean, std, n_noc_test, n_fault_slice, n_slice):\n",
        "    X_train = gen_data_train(mean, std)\n",
        "    X_test = gen_data_test(mean, std, n_noc_test, n_fault_slice, n_slice)\n",
        "    return X_train, X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generating the training and test data\n",
        "X_train, X_test = gen_monovariate_data(mean, std, n_noc_test, n_fault_slice, \n",
        "                                       n_slice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We should check the **normality assumption** (assume that our data follows a Gaussian distribution) of the generated data. A typical way is to plot a quartile-quartile plot and check its linearity. \n",
        "\n",
        "See this [link](https://library.virginia.edu/data/articles/understanding-q-q-plots#:~:text=A%20QQ%20plot%20is%20a,truly%20come%20from%20normal%20distributions.) for furhter information about quartile-quartile plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import pylab\n",
        "\n",
        "sm.qqplot((X_train - X_train.mean(axis=0)) / X_train.std(axis=0), line='45')\n",
        "pylab.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is also important to visualize the behaviour of normal data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time-series plot\n",
        "x = np.linspace(X_train.min() - 2, X_train.max() + 2, 1000)\n",
        "plt.plot(X_train)\n",
        "plt.title('Normal operation data against time')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Reactor Temperature')\n",
        "plt.show()\n",
        "\n",
        "# Histogram\n",
        "plt.hist(X_train, 20, density=True, facecolor='g', alpha=0.75, label='data')\n",
        "plt.plot(x, scipy.stats.norm.pdf(x, mean, std), color='black', label='normal distribution')\n",
        "plt.xlabel('Temperature')\n",
        "plt.xlim(min(x), max(x))\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwELJeb620IA"
      },
      "source": [
        "We can select a specific threshold over which any value will be considered an anomaly. A typical one is $2\\sigma$, which corresponds with about a 95 % confidence level\n",
        "\n",
        "Note that, in real-life examples, we do not know the exact mean and standard deviation of our data. Therefore, the way to work with them is estimating the mean and standard deviation of our sample in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "DZapnodm2mwh",
        "outputId": "f8395b59-4cd6-4da4-972b-e45a7731e5d9"
      },
      "outputs": [],
      "source": [
        "mean_train = X_train.mean()\n",
        "std_est = X_train.std()\n",
        "th_lower = mean_train - 2 * std_est\n",
        "th_higher = mean_train + 2 * std_est\n",
        "print(f'Mean: {mean_train:.2f}')\n",
        "print(f'Standard deviation: {std_est:.2f}')\n",
        "print(f'Thresholds. Lower limit: {th_lower:.2f}. Upper limit: {th_higher:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize how these thresholds look like in the probability density function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Gvg37cyh2x9p",
        "outputId": "584dacd7-807b-4472-fbd9-780da2fc0406"
      },
      "outputs": [],
      "source": [
        "# Get NOC bounds\n",
        "x_fill = np.linspace(th_lower, th_higher)\n",
        "fig_pdf, ax_pdf = plt.subplots()\n",
        "ax_pdf.plot(x, scipy.stats.norm.pdf(x, mean, std), color='black', label='normal distribution')\n",
        "ax_pdf.fill_between(x_fill, scipy.stats.norm.pdf(x_fill, mean, std), color='grey')\n",
        "# Get anomaly bounds\n",
        "x_fault = np.linspace(min(x), th_lower)\n",
        "ax_pdf.fill_between(x_fault, scipy.stats.norm.pdf(x_fault, mean, std), color='salmon')\n",
        "x_fault = np.linspace(th_higher, max(x))\n",
        "ax_pdf.fill_between(x_fault, scipy.stats.norm.pdf(x_fault, mean, std), color='salmon')\n",
        "ax_pdf.set_xlim(mean - 10, mean + 10)\n",
        "ax_pdf.set_ylim([0, None])\n",
        "ax_pdf.set_xlabel('Temperature')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's visualize the faulty data (`X_test`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time series plot\n",
        "x_fault = np.arange(1, len(X_test) + 1)\n",
        "plt.plot(x_fault, X_test)\n",
        "plt.axvline(n_noc_test, color='salmon', label='fault starts')\n",
        "\n",
        "plt.plot([x_fault[0], x_fault[-1]], [th_higher, th_higher], 'r--')\n",
        "plt.plot([x_fault[0], x_fault[-1]], [th_lower, th_lower], 'r--')\n",
        "\n",
        "plt.title('Anomaly generation against time')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Reactor Temperature')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Histogram\n",
        "ax_pdf.hist(X_test, 20, density=True, facecolor='g', alpha=0.75, label='anomaly data')\n",
        "ax_pdf.legend(loc='best')\n",
        "fig_pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Performance on cross-validation\n",
        "\n",
        "**Hands on:** Now is your turn to assess how good your model is. You need to write python code to obtain the predicted labels (normal or faulty). The predicted probability of a data point must be whithin the lower and upper thresholds in order to be considered normal data. Plot the predictions and discuss the results. Also, obtain a metric for how well does this anomaly detector worked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "####### START YOUR CODE HERE #######\n",
        "\n",
        "\n",
        "####### END YOUR CODE HERE #########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "    <summary><font color=\"#DE063A\">Click here to hide/unhide the answer!</font></summary>\n",
        "\n",
        "    # First we get the actual labels of our cross-validation set\n",
        "    # We know that the first 100 instances are normal, while the fault starts\n",
        "    #  from that point onwards\n",
        "    y_true = np.concatenate((\n",
        "        np.zeros((n_noc_test,)),\n",
        "        np.ones((n_fault_slice * n_slice))\n",
        "    ))\n",
        "    # Then we get the predicted labels.\n",
        "    y_pred = (X_test > th_higher) | (X_test < th_lower)\n",
        "\n",
        "    # Plot our prediction\n",
        "    plt.plot(x_fault[y_pred == 0], X_test[y_pred == 0], 'b.', label='predicted as normal')\n",
        "    plt.plot(x_fault[y_pred == 1], X_test[y_pred == 1], 'rx', label='predicted as anomaly')\n",
        "    plt.title('Anomalies detected over time')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Reactor Temperature')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    # Use the accuracy_score function from skit-learn\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f\"The obtained accuracy is {accuracy * 100:0.2f} %\")\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that due to our threshold, a few false positives take place. This is a common trade-off in anomaly detection:\n",
        "- **It is usually difficult to avoid both false positives and false negatives** at the same time\n",
        "- The determination of the threshold is an important task that the engineer must take into account to give more weight to one or another.\n",
        "- This depends heavily on the application: e.g. in medical applications, false positives are highly discouraged, while spam detection might be more indulgent in that sense\n",
        "\n",
        "Also note that the fault takes quite a while to manifest, from it start at $t=100$ min to the first visually noticeable drift at aroun $t=300$ min. More advanced methods might help us anticipate this anomaly but, without more variables showing more hints, it can be impossible to increase our accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sJovavKqG0U"
      },
      "source": [
        "# 2. Probabilistic approach - Multivariate example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGsiNIJ6qMf4"
      },
      "source": [
        "Now, consider a more complex case with generic 10 variables that conform a dataset representative of the state of the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncIVzRVDwDCa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "np.random.seed(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Data generation\n",
        "\n",
        "The function `generate_multivariate_normal_dataset` produces the normal and faulty multivariate data. This function returns a dataframe and a numpy array with the data. The function takes the following inputs:\n",
        "- `mean`: mean vector of the distribution in normal operation conditions.\n",
        "- `covariance`: covariance matrix of the distribution in normal operation conditions.\n",
        "- `n`: total number of points to generate.\n",
        "- `perc_noc`: percetange of the normal data points that are in normal operation conditions. \n",
        "- `changing_rate`: changing rate. Needed for generating the faulty data.\n",
        "\n",
        "Optionally you can provide the random state (`rng`) and if you want to shuffle the data (`shuffle`).\n",
        "\n",
        "**Note**: Take your time to understand how the data is generated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IwHfD1AqFzN"
      },
      "outputs": [],
      "source": [
        "def generate_multivariate_normal_dataset(mean, covariance, n, perc_noc, changing_rate, rng=None, shuffle=False):\n",
        "  n_noc = int (n * perc_noc)\n",
        "  n_fault = n - n_noc\n",
        "  x_noc = np.zeros((n_noc, 10))\n",
        "  x_fault = np.zeros((n_fault, 10))\n",
        "  cols = ['x1', 'x2', 'x3', 'x4','x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'label']\n",
        "\n",
        "  # Generate data under normal conditions\n",
        "  x_noc = rng.multivariate_normal(mean=mean, cov=covariance, size=n_noc)\n",
        "  label_noc = np.zeros(n_noc)\n",
        "  label_noc = label_noc.reshape((len(label_noc), 1))\n",
        "  data_noc = np.concatenate((x_noc, label_noc), axis=1)\n",
        "\n",
        "  # Generate data include fault\n",
        "  poisson_generator = rng.poisson(0.2, n_fault)\n",
        "  label_poisson = np.ones(n_fault)\n",
        "  for j in range(n_fault):\n",
        "    if poisson_generator[j] != 0:\n",
        "        label_poisson[0:j+1] = 0\n",
        "        break\n",
        "\n",
        "  # Depends on whether mean or cov is changing\n",
        "  mean_fault = mean\n",
        "  cov_fault = covariance\n",
        "\n",
        "  for i in range(x_fault.shape[0]):\n",
        "    if poisson_generator[i] != 0:\n",
        "        cov_fault[0,1] *= (1+changing_rate)**(poisson_generator[i])\n",
        "        cov_fault[1,0] *= (1+changing_rate)**(poisson_generator[i])\n",
        "    instance = rng.multivariate_normal(mean=mean_fault, cov=cov_fault, size=1)\n",
        "    x_fault[i] = instance\n",
        "\n",
        "  label_poisson = label_poisson.reshape((len(label_poisson), 1))\n",
        "  data_fault = np.concatenate((x_fault, label_poisson), axis=1)\n",
        "\n",
        "  data_np = np.concatenate((data_noc, data_fault), axis=0)\n",
        "  data = pd.DataFrame(data_np, columns=cols)\n",
        "\n",
        "  if shuffle == True:\n",
        "    data = data.sample(frac=1, random_state=rng)# .reset_index(drop=True)\n",
        "    data_np = data_np[data.index]\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  return data, data_np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will use the `generate_multivariate_normal_dataset` function to generate the training and testing data sets of the multivariate distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CzmK7cowFfb",
        "outputId": "68e77837-f700-42c5-f3fc-bc660c5d5fda"
      },
      "outputs": [],
      "source": [
        "# Generate data\n",
        "n_samples = 500\n",
        "perc_noc_train = 1   # % NOC instances in the training set (100 %)\n",
        "perc_noc_test = 0.2  # % NOC instances in the test set (100 %)\n",
        "changing_rate = 0.15\n",
        "base_rng      = np.random.default_rng(42)\n",
        "\n",
        "\n",
        "# Generate means\n",
        "mean = base_rng.integers(low=1, high=10, size=10)\n",
        "print('The mean of the distribution in NOC is: ', mean)\n",
        "\n",
        "# Generate covariance\n",
        "covariance = np.array([[2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5],\n",
        "                      [-0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5],\n",
        "                      [0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5],\n",
        "                      [-0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5],\n",
        "                      [0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5],\n",
        "                      [-0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5],\n",
        "                      [0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5],\n",
        "                      [-0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5],\n",
        "                      [0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5],\n",
        "                      [-0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0]])\n",
        "print('The covariance of the distribution in NOC is :', covariance)\n",
        "\n",
        "# Generate numpy data and do train-test split\n",
        "data_train, data_train_np = generate_multivariate_normal_dataset(\n",
        "    mean, covariance, n_samples, perc_noc_train, changing_rate, rng=base_rng, \n",
        "    shuffle=False)\n",
        "data_test, data_test_np = generate_multivariate_normal_dataset(\n",
        "    mean, covariance, n_samples, perc_noc_test, changing_rate, rng=base_rng, \n",
        "    shuffle=False)\n",
        "X_train = data_train.drop(['label'], axis=1).to_numpy()\n",
        "y_train = data_train['label'].to_numpy()\n",
        "X_test = data_test.drop(['label'], axis=1).to_numpy()\n",
        "y_test = data_test['label'].to_numpy()\n",
        "\n",
        "# The features of the dataset correspond to the columns of X_train\n",
        "n_features = X_train.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYBLnEGXzUQm"
      },
      "source": [
        "In this case, the dataset is completly random, and disturbances are introduced in the covariance between the first and second variable, which starts suffering a drift based on a Poisson disturbance like in the single-variable example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "LS4t04AdyNMT",
        "outputId": "41d12fb9-7acc-49bb-ef3d-1ab01f2327b0"
      },
      "outputs": [],
      "source": [
        "plt.plot(data_test.iloc[:300,0], label='test')\n",
        "plt.plot(data_train.iloc[:300,0], label='train')\n",
        "plt.legend(loc='best')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Anomaly detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to estimate the detection thresholds for each variable separately. For this, we use the probability density function of the normal distribution\n",
        "\n",
        "$$ f(x_i|\\mu, \\Sigma) = \\frac{1}{{\\sqrt{(2 \\pi)^n | \\Sigma} |}} e^{-\\frac{1}{2}{(x - \\mu)^T \\Sigma^{-1} {(x - \\mu)}}}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the probability density function is not exactly yielding probabilities, but a measure of the density of the random distribution at a specific point. However, it can be used to locate points in the probability distrubution ranges.\n",
        "\n",
        "**Hands on:** Write a function called `probability_density` that computes the probability density function of the normal distribution function. This function should take a `X`, `mean` and `var` vectors as inputs and should return the probability density evaluation. **Make sure your function works for onedimensional and multdimensional distributions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "####### START YOUR CODE HERE #######\n",
        "\n",
        "\n",
        "####### END YOUR CODE HERE #########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "\n",
        "    def probability_density(X, mean, var):\n",
        "        \"\"\"\n",
        "        Calculate the probability density of a series of instances in X given the\n",
        "        multivariate mean and covariance matrix.\n",
        "        \"\"\"\n",
        "        n = len(mean)\n",
        "        if var.ndim == 1:\n",
        "            var = np.diag(var)\n",
        "        X = X - mean\n",
        "        p = (2 * np.pi)**(- n/2) * np.linalg.det(var)**(-0.5) * \\\n",
        "            np.exp(-0.5 * np.sum(np.matmul(X, np.linalg.pinv(var)) * X, axis=1))\n",
        "        if len(p) == 1:\n",
        "            return p[0]\n",
        "        else:\n",
        "            return p\n",
        "        \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we estimate the means and covariance of our system using the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trGRozQC5nBD"
      },
      "outputs": [],
      "source": [
        "# Estimate Gaussian distribution\n",
        "mean_train = np.mean(X_train, axis=0)\n",
        "cov_train = np.cov(X_train, rowvar=False)  # By default it deals with each row as a variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hands on:** design and write a function called `get_threshold` to obtain a threshold given a desired target accuracy. The function should accept the the scores (`scores`), the real labels (`y_true`), the target accuracy (`target_acc`). Additionally, add the option `positive_lower` that if True then positive instances are predicted when the score is lower than the threshold. \n",
        "\n",
        "A simple procedure to obtain the threshold consists of systematically evaluating a range of threshold untilyou reach approximately the desired accuracy. The prodecure is as follows:\n",
        "1. Create a range of `n_samples` values of the scores from the minimum to the maximum values using. You will iterate over this range.\n",
        "2. Select a value from the range and set the threshold to that value.\n",
        "3. Obtain the predictions based on the current threshold.\n",
        "4. Obtain the accuracy of the model based on the current threshold.\n",
        "5. If the accuracy is lower than the desired value then move on to the following value of the score range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "####### START YOUR CODE HERE #######\n",
        "\n",
        "\n",
        "####### END YOUR CODE HERE #########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "    <summary><font color=\"#DE063A\">Click here to hide/unhide the answer!</font></summary>\n",
        "\n",
        "        def get_threshold(scores, y_true, target_acc=0.95, positive_lower=True, \n",
        "                        n_samples=1e4, print_stdout=True, plot=True):\n",
        "            \"\"\" \n",
        "            Obtain threshold by searching for the closest accuracy to target_acc via \n",
        "            brute force.\n",
        "            ----------\n",
        "            Parameters\n",
        "            ----------\n",
        "            positive_lower: if True, positive instances are predicted when the score\n",
        "                is lower than the threshold. If False, positive instances are predicted\n",
        "                for scores higher than the threshold\n",
        "            \"\"\"\n",
        "            # Get extremes of scores\n",
        "            scores_range = np.linspace(scores.min(), scores.max(), int(n_samples))\n",
        "            # Loop them until finding the closer value to 95 % accuracy (brute force)\n",
        "            error_old = 1\n",
        "            accs = {}\n",
        "            for score_th in scores_range:\n",
        "                # Obtain predictions given the current threshold\n",
        "                if positive_lower:\n",
        "                    y_pred = scores < score_th\n",
        "                else:\n",
        "                    y_pred = scores > score_th\n",
        "                # Calculate accuracy and measure the error with respect target_acc\n",
        "                acc = accuracy_score(y_true, y_pred)\n",
        "                accs[score_th] = acc\n",
        "                error = (target_acc - acc) ** 2\n",
        "                if print_stdout:\n",
        "                    print(f\"Obtained accuracy {acc}, error {error}\" \n",
        "                        f\" with threshold = {score_th}\")\n",
        "                if error < error_old:\n",
        "                    threshold = score_th \n",
        "                    error_old = error\n",
        "                elif error > error_old:\n",
        "                    # Avoid continuing since no better error will be obtained\n",
        "                    break\n",
        "            if plot:\n",
        "                plt.figure()\n",
        "                plt.plot(pd.Series(accs), label='evolution of accuracy')\n",
        "                plt.vlines(threshold, plt.gca().get_ylim()[0], 1, color='red', label='final threshold')\n",
        "                plt.xlabel('Iteration')\n",
        "                plt.ylabel('Accuracy')\n",
        "                plt.legend(loc='best')\n",
        "            return threshold\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now use the `probability_density` function to obtain the model's predictions and when obtain the threshold using the `get_threshold` function on the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get probabilities and threshold\n",
        "probs = probability_density(X_train, mean_train, cov_train)\n",
        "threshold = get_threshold(probs, y_train, positive_lower=True, n_samples=1e5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, check if we obtain the desired accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_pred = probability_density(X_train, mean_train, cov_train) < threshold\n",
        "print(f'We should have close to {int(len(X_train) * 0.05)} false positives.')\n",
        "print(f'We have {sum(y_train_pred)} false positives')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our threshold, we can evaluate the test set. Remember we use the fitted mean and variance from the training set, we do not obtain these from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_test = probability_density(X_test, mean_train, cov_train) \n",
        "y_test_pred = p_test < threshold\n",
        "acc_test = accuracy_score(y_test, y_test_pred)\n",
        "print(f'The obtained accuracy is {acc_test *100} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feel free to play arround with different values of desired accuracy. Discuss how the predictions of the model change. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Proximity-based approaches - Clustering example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we are going to work with the iris dataset, which has become famous among the pattern recognition literature. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is linearly separable from the other 2; the latter are not linearly separable from each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Iris dataset](https://upload.wikimedia.org/wikipedia/commons/c/cb/Flores_de_%C3%8Dris.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Exercise**: Use k-means clustering to perform anomaly detection on the iris dataset.\n",
        "\n",
        "- The training set consists on the 'setosa' and 'virginica' (first and third class) data points.\n",
        "- The test set consists on the 'versicolor' instances.\n",
        "- As a simplification, use only the first and the last features of the dataset: the sepal length and the petal width.\n",
        "\n",
        "New instances should be classified as an anomaly if they do not belong to one of the known labels: 'setosa' or 'virginica'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "print('Features: ', iris.feature_names)\n",
        "print('Iris flower names:', iris.target_names)\n",
        "\n",
        "n_clusters = 2  # We assume our training set is composed of 2 clusters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is a good practice to visualize how your data looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the data\n",
        "fig_orig, ax_orig = plt.subplots()\n",
        "scatter = ax_orig.scatter(iris.data[:, 0], iris.data[:, 3], c=iris.target)\n",
        "ax_orig.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[3])\n",
        "ax_orig.legend(scatter.legend_elements()[0], iris.target_names, \n",
        "          loc=\"lower right\", title=\"Classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we will train a k-means with clusters. First, we need to split the data into a training and testing data set.\n",
        "\n",
        "For further information about the k-means model, check the official [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform train-test split\n",
        "y = iris.target\n",
        "X_train = iris.data[:, [0, 3]][y != 1]  # [0,3] to get first and fourth feature\n",
        "                                        # [y != 1] to get all classes except 1\n",
        "X_test = iris.data[:, [0, 3]][y == 1]   # Same for the test set\n",
        "y_train = np.zeros((len(X_train),))\n",
        "y_test = np.ones((len(X_test),))\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit k-means model with two clusters\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans.fit(X_train)\n",
        "kmeans.labels_, kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cluster predictions\n",
        "_, ax = plt.subplots()\n",
        "scatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=kmeans.labels_)\n",
        "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[3])\n",
        "ax.legend(scatter.legend_elements()[0], iris.target_names, \n",
        "          loc=\"lower right\", title=\"Classes\")\n",
        "print('We can see that k-means correctly separates the two classes except for one single point')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have fitted our model we can try to predict anomalies. The algorithm would go as follows. For any new instance:\n",
        "\n",
        "1. Associate the new instance with the closest cluster centroid.\n",
        "2. Calculate a probabilistic/distance-based anomaly metric\n",
        "3. Classify the new instance as normal or anomaly depending on a threshold previously selected\n",
        "\n",
        "**Hands on:** obtain the threshold for every cluster. You can use the previous coded `get_threshold` function in the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "####### START YOUR CODE HERE #######\n",
        "\n",
        "\n",
        "####### END YOUR CODE HERE #########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "    <summary><font color=\"#DE063A\">Click here to hide/unhide the answer!</font></summary>\n",
        "\n",
        "        th_clusters = []\n",
        "        mean_clusters = []\n",
        "        cov_clusters = []\n",
        "        for i in range(n_clusters):\n",
        "            X_train_cluster = X_train[kmeans.labels_ == i]\n",
        "            y_train_cluster = np.zeros(len(X_train_cluster))\n",
        "            # Estimate Gaussian distribution of each cluster\n",
        "            mean_train = np.mean(X_train_cluster, axis=0)\n",
        "            mean_clusters.append(mean_train)\n",
        "            cov_train = np.cov(X_train_cluster, rowvar=False)\n",
        "            cov_clusters.append(cov_train)\n",
        "            # Get probabilities and threshold\n",
        "            probs = probability_density(X_train_cluster, mean_train, cov_train)\n",
        "            th = get_threshold(probs, y_train_cluster, positive_lower=True, \n",
        "                            n_samples=1e4, print_stdout=False, plot=False)\n",
        "            th_clusters.append(th)\n",
        "        print('Thresholds are', th_clusters)\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hands on:** Now that you have computed the threshold for each cluster, you can compute the accuracy of your model. Save the predictions on the `y_train_pred` variables and scores on the `scores` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "####### START YOUR CODE HERE #######\n",
        "\n",
        "\n",
        "####### END YOUR CODE HERE #########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "    <summary><font color=\"#DE063A\">Click here to hide/unhide the answer!</font></summary>\n",
        "\n",
        "    y_train_pred = np.zeros(len(X_train))\n",
        "    scores = np.zeros(len(X_train))\n",
        "    for i in range(len(X_train)):\n",
        "        # Reshape instance to make it 2-dimensional\n",
        "        x = X_train[i, :].reshape(1, -1)\n",
        "        cluster_idx = kmeans.labels_[i]\n",
        "        # print(f'Point {i} belongs to cluster {cluster_idx}')\n",
        "        # Calculate the Mahalanobis distance\n",
        "        scores[i] = probability_density(\n",
        "            x, mean_clusters[cluster_idx], cov_clusters[cluster_idx])\n",
        "        # Predict anomaly\n",
        "        y_train_pred[i] = scores[i] < th_clusters[cluster_idx]\n",
        "    print(f'The accuracy on the train set is {accuracy_score(y_train, y_train_pred) * 100} %')\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "scatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train_pred)\n",
        "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[3])\n",
        "ax.legend(scatter.legend_elements()[0], ['normal', 'anomaly'], \n",
        "          loc=\"lower right\", title=\"Classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Repeat the process for the test set. For new instances (test set) you first need to determine closest cluster and compare distances to the threshold.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "####### START YOUR CODE HERE #######\n",
        "\n",
        "\n",
        "####### END YOUR CODE HERE #########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "    <summary><font color=\"#DE063A\">Click here to hide/unhide the answer!</font></summary>\n",
        "\n",
        "    y_test_pred = np.zeros(len(X_test))\n",
        "    scores = np.zeros(len(X_test))\n",
        "    for i in range(len(X_test)):\n",
        "        # Rehape instance to make it 2-dimensional\n",
        "        x = X_test[i, :].reshape(1, -1)\n",
        "        cluster_idx = kmeans.predict(x)[0]\n",
        "        # Calculate the Mahalanobis distance\n",
        "        scores[i] = probability_density(\n",
        "            x, mean_clusters[cluster_idx], cov_clusters[cluster_idx])\n",
        "        # Predict anomaly\n",
        "        y_test_pred[i] = scores[i] < th_clusters[cluster_idx]\n",
        "    print(f'The accuracy on the test set is {accuracy_score(y_test, y_test_pred) * 100} %')\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare our anomaly detection results with the actual classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test_pred)\n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with the original classes\n",
        "fig_orig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Neural networks: autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neural networks are usually trained in a supervised manner. To be able to use them in a semisupervised problem, we cannot use the typical feedforward neural network. For this, the self-associative neural networks, also called autoencoders, come to help. Their name is self-explanatory: the goal of autoencorders is to try to reproduce the input in the output of the neural network. This may look like a trivial task but the trick here is that there is a bottleneck in the architecture of the network that causes a loss of information.\n",
        "\n",
        "The training phase tries to make the reconstruction with the lowest possible loss despite the bottleneck. This way it is able to find the most important features and discard noise that is useless to reconstruct the input. Its usefulness in anomaly detection comes with that, if trained with normal operation data and learns to identify and reconstruct it correctly when an abnormal event occurs, the reconstruction will show an important bias compared with nominal operation data.\n",
        "\n",
        "Therefore, they do not use labels to train the network. However, we need to know that all the instances fed during training belong to the same class (usually the *normal operation* class), therefore the *semisupervised* surname of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Exercise**: We will work with the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset. For this, we will try to identify as an anomaly every number that is not a \"5\"\n",
        "\n",
        "![MNIST dataset](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we will load the dataset from the scikit-learn builtin datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Import tensorflow to download higher resolution mnist dataset\n",
        "# Otherwise sklearn will be used\n",
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_mnist(X, y, n=3, title=''):\n",
        "    \"\"\" Distribute plots in rows of 4. \"\"\"\n",
        "    import math\n",
        "    # Get number of pixels in each dimension\n",
        "    side_px = int(math.sqrt(X.shape[1]))\n",
        "    nrows = math.ceil(n / 4)\n",
        "    ncols = 4\n",
        "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 10))\n",
        "    i = 0\n",
        "    for image, label in zip(X, y):\n",
        "        #np.unravel_index(i, axes.shape)\n",
        "        ax = axes.flatten()[i]\n",
        "        ax.set_axis_off()\n",
        "        image = image.reshape(side_px, side_px)\n",
        "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "        if not label:\n",
        "            ax.set_title(f\"Label: {bool(label)}\", fontweight=\"bold\")\n",
        "        else:\n",
        "            ax.set_title(f\"Label label: {bool(label)}\")\n",
        "        i += 1\n",
        "        if i >= n:\n",
        "            break\n",
        "    fig.suptitle(title)\n",
        "\n",
        "\n",
        "def get_mnist_5(split=False):\n",
        "    # Load data\n",
        "    try:\n",
        "        (X_train, y_train), (X_test, y_test) = \\\n",
        "            tf.keras.datasets.mnist.load_data()\n",
        "        # Unroll pixels to follow sklearn setup\n",
        "        X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "        X_test = X_test.reshape((X_test.shape[0], -1))\n",
        "        # Set up labels to this specific case\n",
        "        y_train = ~(y_train == 5)\n",
        "        y_test = ~(y_test == 5)\n",
        "        return X_train, X_test, y_train, y_test\n",
        "    except:\n",
        "        digits = datasets.load_digits()\n",
        "\n",
        "        # Get input and output data\n",
        "        X = digits.data\n",
        "        labels = digits.target\n",
        "        y = ~(labels == 5)\n",
        "\n",
        "        if split:\n",
        "            # Split data into 50% train and 50% test subsets\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.2, shuffle=True\n",
        "            )\n",
        "            return X_train, X_test, y_train, y_test\n",
        "        else:\n",
        "            return X, y\n",
        "    \n",
        "    \n",
        "X_train, X_test, y_train, y_test = get_mnist_5(split=True)\n",
        "plot_mnist(X_train, y_train, n=12)\n",
        "print(\"Five's are represented as False (normal), \"\n",
        "      \"while the rest are considered True (anomaly)\")\n",
        "\n",
        "# Keep only 5's for the training set\n",
        "X_train = X_train[y_train == False]\n",
        "y_train = y_train[y_train == False]\n",
        "plot_mnist(X_train, y_train, n=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each instance of the training set consists of 64 elements that represent the 8-by-8 greyscale pixel image of a handwritten number. Each pixel value ranges from 0 to 16, from white to black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('This is an example of the contents of an image.'\n",
        "      ' The zero-valued pixels correspond to white background')\n",
        "print(X_train[1, :], y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before working with the data, we need to standardize the pixel intensisty values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scaling (no need of centering in images)\n",
        "scale_factor = X_train.max()\n",
        "X_train_scaled = X_train / scale_factor\n",
        "X_test_scaled = X_test / scale_factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have our dataset, we can set up our autoencoder neural network.\n",
        "\n",
        "First, set up the architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shape of input and latent variable\n",
        "n_input = X_train.shape[1]\n",
        "\n",
        "# Encoder structure\n",
        "n_encoder1 = 10\n",
        "# n_encoder2 = 10\n",
        "n_latent = 2\n",
        "\n",
        "# Decoder structure\n",
        "# n_decoder2 = 10\n",
        "n_decoder1 = 10\n",
        "\n",
        "hidden_layer_sizes = (n_encoder1, n_latent, n_decoder1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize the model using sklearn's [`MLPRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) (Multilayer Perceptron)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = MLPRegressor(\n",
        "    hidden_layer_sizes=hidden_layer_sizes, \n",
        "    activation='tanh', \n",
        "    solver='adam', \n",
        "    batch_size='auto', \n",
        "    learning_rate_init=0.001,\n",
        "    max_iter=1000,\n",
        "    random_state=None,\n",
        "    tol=0.0000001,\n",
        "    verbose=True,\n",
        "    alpha=0.0001\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can fit the model. Bear in mind that we want the model to try to obtain a target output as similar as possible to the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X_train_scaled, X_train_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(model.loss_curve_)\n",
        "plt.xlabel('Training epochs')\n",
        "plt.ylabel('Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check how reconstructions go for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_inout_mnist(image_in, image_out):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
        "    axes[0].imshow(image_in, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "    axes[0].set_title(\"Input\", fontsize=26)\n",
        "    axes[0].axis('off')\n",
        "    axes[1].imshow(image_out, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "    axes[1].set_title(\"Output\", fontsize=26)\n",
        "    axes[1].axis('off')\n",
        "    plt.show()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 5\n",
        "side_px = int(np.sqrt(X_train.shape[1]))\n",
        "input_image = X_train_scaled[0, :].reshape((side_px,  side_px))\n",
        "# Perform reconstruction through autoencoder\n",
        "reconstructed_image = model.predict(X_train_scaled[0:0 + 1, :])\n",
        "side_px = int(np.sqrt(X_train.shape[1]))\n",
        "reconstructed_image = reconstructed_image.reshape((side_px, side_px))\n",
        "compare_inout_mnist(input_image, reconstructed_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot other numbers in the test set\n",
        "reconstructed_images = []\n",
        "for i in range(1, 5):\n",
        "    input_image = X_test_scaled[i, :].reshape((side_px,  side_px))\n",
        "    # Perform reconstruction through autoencoder\n",
        "    reconstructed_image = model.predict(X_test_scaled[i:i + 1, :])\n",
        "    reconstructed_image = reconstructed_image.reshape((side_px, side_px))\n",
        "    compare_inout_mnist(input_image, reconstructed_image)\n",
        "    reconstructed_images.append(reconstructed_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The anomaly detection algorithm to detect new instances as normal or outliers and obtain a metric of performance is similar to previous exercises. This consists on the following:\n",
        "\n",
        "1. Selecting a useful metric (e.g., the reconstruction error)\n",
        "2. Based on the training set, obtain a threshold to satify a given accuracy of the anomaly detection algorithm.\n",
        "3. Get the accuracy of the model on unseen data.\n",
        "\n",
        "**Hands on:** The student is left to complete this part of the problem. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "####### START YOUR CODE HERE #######\n",
        "\n",
        "\n",
        "####### END YOUR CODE HERE #########"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
